{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ-AU5TlDFP3"
      },
      "source": [
        "# **Operations to be done with Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19amJVo4y8KR",
        "outputId": "a876290f-918d-4253-89e1-65053d6b7e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Linking with Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zHp0WVQDl24"
      },
      "source": [
        "# **Prerequisite Operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NV0XOtvZ7S05"
      },
      "outputs": [],
      "source": [
        "#@title Installing Dependencies\n",
        "\n",
        "%pip install librosa\n",
        "!apt install ffmpeg\n",
        "%pip install pydub\n",
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-Q5TkH3ibKMd"
      },
      "outputs": [],
      "source": [
        "#@title Requirements\n",
        "\n",
        "import tensorflow as tf\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iu9_dwODdj-"
      },
      "source": [
        "# **Data Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AmKwx57kSig_"
      },
      "outputs": [],
      "source": [
        "#@title Data Conversion\n",
        "\n",
        "from pydub import AudioSegment\n",
        "from tqdm import tqdm\n",
        "\n",
        "raw_data = ['/content/drive/My Drive/Raga Identification/Thaat']\n",
        "print(\"Converting thaat songs from other formats to .wav\")\n",
        "\n",
        "for thaat in os.listdir(raw_data[0]):\n",
        "    for audio_class in tqdm(os.listdir(os.path.join(raw_data[0], thaat)), desc=thaat):\n",
        "        for audio in os.listdir(os.path.join(raw_data[0], thaat, audio_class)):\n",
        "            audio_path = os.path.join(raw_data[0], thaat, audio_class, audio)\n",
        "            if \".mp3\" in audio_path:\n",
        "                sound = AudioSegment.from_mp3(audio_path)\n",
        "                sound.export((audio_path[:-3]+\"wav\"), format=\"wav\")\n",
        "                os.remove(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LBt93jrdRxZ6"
      },
      "outputs": [],
      "source": [
        "#@title Audio Data Split\n",
        "\n",
        "from pydub import AudioSegment\n",
        "import math\n",
        "import gc\n",
        "\n",
        "class SplitWavAudio():\n",
        "    def __init__(self, folder, filename):\n",
        "        self.folder = folder\n",
        "        self.filename = filename\n",
        "        self.filepath = folder + '/' + filename\n",
        "\n",
        "        self.audio = AudioSegment.from_wav(self.filepath)\n",
        "\n",
        "    def get_duration(self):\n",
        "        return self.audio.duration_seconds\n",
        "\n",
        "    def single_split(self, from_sec, to_sec, split_filename):\n",
        "        t1 = from_sec * 1000\n",
        "        t2 = to_sec * 1000\n",
        "        split_audio = self.audio[t1:t2]\n",
        "        split_audio.export(self.folder + '/' + split_filename, format=\"wav\")\n",
        "\n",
        "    def multiple_split(self, min_per_split):\n",
        "        total_secs = math.ceil(self.get_duration())\n",
        "        for i in range(0, total_secs, int(min_per_split*60)):\n",
        "            split_fn = 'split ' + str(i) + '_' + self.filename\n",
        "            self.single_split(i, i+int(min_per_split*60), split_fn)\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "for thaat in os.listdir(raw_data[0]):\n",
        "    for audio_class in tqdm(os.listdir(os.path.join(raw_data[0], thaat)), desc=thaat):\n",
        "        for audio in os.listdir(os.path.join(raw_data[0], thaat, audio_class)):\n",
        "            obj = SplitWavAudio(os.path.join(raw_data[0], thaat, audio_class), audio)\n",
        "            obj.multiple_split(min_per_split=0.5)\n",
        "            os.remove(os.path.join(raw_data[0], thaat, audio_class, audio))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qWHu9_mz4Fmr"
      },
      "outputs": [],
      "source": [
        "#@title Data Extraction\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "raw_data = ['/content/drive/My Drive/Raga Identification/Thaat']\n",
        "\n",
        "data = '/content/drive/My Drive/Raga Identification/data'\n",
        "os.mkdir(data)\n",
        "\n",
        "for thaat in os.listdir(raw_data[0]):\n",
        "    thaat_path = os.path.join(data, thaat)\n",
        "    os.mkdir(thaat_path)\n",
        "    for audio_class in tqdm(os.listdir(os.path.join(raw_data[0], thaat)), desc=thaat):\n",
        "        spec_path = os.path.join(thaat_path, audio_class)\n",
        "        os.mkdir(spec_path)\n",
        "        for audio in os.listdir(os.path.join(raw_data[0], thaat, audio_class)):\n",
        "            audio_path = os.path.join(raw_data[0], thaat, audio_class, audio)\n",
        "            y, sr = librosa.load(audio_path)\n",
        "            D = librosa.stft(y)\n",
        "            S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "            M = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "            M_db = librosa.power_to_db(M, ref=np.max)\n",
        "            chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
        "            del y; del sr; del D; del M;\n",
        "            gc.collect()\n",
        "\n",
        "            fig, ax = plt.subplots(nrows=3, ncols=1, sharex=True)\n",
        "\n",
        "            img1 = librosa.display.specshow(S_db, x_axis='time', y_axis='log', ax=ax[0])\n",
        "            ax[0].set(title='STFT (log scale)')\n",
        "\n",
        "            img2 = librosa.display.specshow(M_db, x_axis='time', y_axis='mel', ax=ax[1])\n",
        "            ax[1].set(title='Mel Spectrogram')\n",
        "\n",
        "            img3 = librosa.display.specshow(chroma, x_axis='time', y_axis='chroma', key='Eb:maj', ax=ax[2])\n",
        "            ax[2].set(title='Chromagram')\n",
        "\n",
        "            for ax_i in ax:\n",
        "                ax_i.label_outer()\n",
        "\n",
        "            fig.set_figwidth(20)\n",
        "            fig.set_figheight(10)\n",
        "\n",
        "            plt.subplots_adjust()\n",
        "            del S_db; del M_db; del chroma;\n",
        "\n",
        "            fig.savefig(spec_path+\"/\"+audio+\".png\")\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "            gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n1MfCYd-dKzL"
      },
      "outputs": [],
      "source": [
        "#@title Data Cleansing\n",
        "\n",
        "import cv2\n",
        "import imghdr\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "data_dir = ['/content/drive/My Drive/Raga Identification/data']\n",
        "image_exts = ['jpg', 'jpeg', 'png', 'bmp', 'dib', 'jpe', 'jp2', 'webp', 'avif','pbm', 'pgm', 'ppm' 'pxm', 'pnm', 'pfm', 'sr', 'ras', 'tiff', 'tif', 'exr', 'hdr', 'pic']\n",
        "print(os.listdir(data_dir[0]))\n",
        "print(os.listdir(os.path.join(data_dir[0], 'benign')))\n",
        "\n",
        "for thaat in os.listdir(data_dir[0]):\n",
        "    for audio_class in tqdm(os.listdir(os.path.join(data_dir[0], thaat)), desc=thaat):\n",
        "        for audio in os.listdir(os.path.join(data_dir[0], thaat, audio_class)):\n",
        "            image_path = os.path.join(data_dir[0], thaat, audio_class, audio)\n",
        "            try:\n",
        "                img = cv2.imread(image_path)\n",
        "                tip = imghdr.what(image_path)\n",
        "                if tip not in image_exts:\n",
        "                    print('Image not in ext list {}'.format(image_path))\n",
        "                    os.remove(image_path)\n",
        "            except Exception as e:\n",
        "                print('Issue with image {}'.format(image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SK8Ajrcwe37G"
      },
      "outputs": [],
      "source": [
        "#@title Creating Data Pipeline\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "data = tf.keras.utils.image_dataset_from_directory('/content/drive/My Drive/Raga Identification/data', shuffle=True)\n",
        "data_iterator = data.as_numpy_iterator()\n",
        "batch = data_iterator.next()\n",
        "print(\"Batch 1 classes: \", batch[1], \"\\n\")\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "for idx, img in enumerate(batch[0][:4]):\n",
        "    ax[idx].imshow(img.astype(int))\n",
        "    ax[idx].title.set_text(batch[1][idx])\n",
        "\n",
        "datas = {}\n",
        "data_iterators = {}\n",
        "batches = {}\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    datas[i] = tf.keras.utils.image_dataset_from_directory(os.path.join(data_dir[0], i), shuffle=True)\n",
        "    data_iterators[i] = datas[i].as_numpy_iterator()\n",
        "    batches[i] = data_iterators[i].next()\n",
        "    print(\"Batch classes for \", i, \" : \", batches[i][1], \"\\n\")\n",
        "    fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "    for idx, img in enumerate(batches[i][0][:4]):\n",
        "        ax[idx].imshow(img.astype(int))\n",
        "        ax[idx].title.set_text(batches[i][1][idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E4fbLQp3fDas"
      },
      "outputs": [],
      "source": [
        "#@title Preprocessing Data\n",
        "\n",
        "''' Scaling Data '''\n",
        "# Scaling Data\n",
        "data = data.map(lambda x,y: (x/255, y))\n",
        "data.as_numpy_iterator().next()[0].max()\n",
        "\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    datas[i] = datas[i].map(lambda x,y: (x/255, y))\n",
        "    print(\"For \"+i+\": \", datas[i].as_numpy_iterator().next()[0].max())\n",
        "    break\n",
        "\n",
        "\n",
        "''' Splitting Data '''\n",
        "train_size = int(len(data)*.8)\n",
        "val_size = int(len(data)*.1)+1\n",
        "test_size = int(len(data)*.1)+1\n",
        "\n",
        "train = data.take(train_size)\n",
        "val = data.take(val_size)\n",
        "test = data.take(test_size)\n",
        "\n",
        "trains = {}\n",
        "tests = {}\n",
        "vals = {}\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    train_size = int(len(datas[i])*.8)\n",
        "    val_size = int(len(datas[i])*.1)+1\n",
        "    test_size = int(len(datas[i])*.1)+1\n",
        "    print(\"For \"+i+\" : \", train_size, \" + \", val_size, \" + \", test_size, \" = \", len(datas[i]))\n",
        "\n",
        "    trains[i] = datas[i].take(train_size)\n",
        "    vals[i] = datas[i].take(val_size)\n",
        "    tests[i] = datas[i].take(test_size)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Assigning weights to classes\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "labels = []\n",
        "for batch in train.as_numpy_iterator():\n",
        "    X1, y1 = batch\n",
        "    labels.extend(list(y1))\n",
        "\n",
        "labels = np.array(labels)\n",
        "print(labels)\n",
        "\n",
        "# Calculate class weights for thaat classes\n",
        "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(labels), y = labels)\n",
        "class_weights = dict(zip(np.unique(labels), class_weights))\n",
        "\n",
        "\n",
        "class_weights_r = {}\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    labels = []\n",
        "    for batch in trains[i].as_numpy_iterator():\n",
        "        X1, y1 = batch\n",
        "        labels.extend(list(y1))\n",
        "\n",
        "    labels = np.array(labels)\n",
        "    print(\"For\", i, \":\", labels)\n",
        "\n",
        "    # Calculate class weights for thaat classes\n",
        "    class_weights_r[i] = compute_class_weight(class_weight = 'balanced', classes = np.unique(labels), y = labels)\n",
        "    class_weights_r[i] = dict(zip(np.unique(labels), class_weights_r[i]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "20PJue_tv8M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QjikBwbEKuG"
      },
      "source": [
        "# **Deep Learning Models Creation, Training, Evaluating, Saving, Performance Graph etc.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ep5DAHCTfq8L"
      },
      "outputs": [],
      "source": [
        "#@title Deep Models CRNN2D, DenseNet\n",
        "\n",
        "''' Deep Model '''\n",
        "# The Network\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, Dropout, AveragePooling2D, Concatenate, GlobalAveragePooling2D, BatchNormalization, ReLU, Add, SeparableConv2D, DepthwiseConv2D, GRU, LSTM, Permute, Reshape, Activation\n",
        "from tensorflow.keras.applications import DenseNet201\n",
        "\n",
        "\n",
        "def CRNN2D(X_shape, nb_classes):\n",
        "\n",
        "    nb_layers = 4\n",
        "    nb_filters = [64, 128, 128, 128]\n",
        "    kernel_size = (3, 3)\n",
        "    activation = 'elu'\n",
        "    pool_size = [(2, 2), (4, 2), (4, 2), (4, 2),\n",
        "                 (4, 2)]\n",
        "\n",
        "    input_shape = (X_shape[0], X_shape[1], X_shape[2])\n",
        "    frequency_axis = 1\n",
        "    time_axis = 2\n",
        "    channel_axis = 3\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(BatchNormalization(axis=frequency_axis, input_shape=input_shape))\n",
        "\n",
        "    model.add(Conv2D(nb_filters[0], kernel_size=kernel_size, padding='same',\n",
        "                     data_format=\"channels_last\",\n",
        "                     input_shape=input_shape))\n",
        "    model.add(Activation(activation))\n",
        "    model.add(BatchNormalization(axis=channel_axis))\n",
        "    model.add(MaxPool2D(pool_size=pool_size[0], strides=pool_size[0]))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    for layer in range(nb_layers - 1):\n",
        "        model.add(Conv2D(nb_filters[layer + 1], kernel_size=kernel_size,\n",
        "                         padding='same'))\n",
        "        model.add(Activation(activation))\n",
        "        model.add(BatchNormalization(\n",
        "            axis=channel_axis))\n",
        "        model.add(MaxPool2D(pool_size=pool_size[layer + 1],\n",
        "                               strides=pool_size[layer + 1]))\n",
        "        model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "    model.add(Permute((time_axis, frequency_axis, channel_axis)))\n",
        "    resize_shape = model.output_shape[2] * model.output_shape[3]\n",
        "    model.add(Reshape((model.output_shape[1], resize_shape)))\n",
        "\n",
        "    model.add(GRU(32, return_sequences=True))\n",
        "    model.add(GRU(32, return_sequences=False))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(nb_classes))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet(img_shape, n_classes):\n",
        "    densenet = DenseNet201(input_shape=img_shape, include_top=False, weights='imagenet')\n",
        "    densenet.trainable = False\n",
        "\n",
        "    input = Input(img_shape)\n",
        "    x = densenet(input, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(rate = 0.2)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    output = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(input, output)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "input_shape = (256, 256, 3)\n",
        "n_classes = len(os.listdir(data_dir[0]))\n",
        "\n",
        "model = CRNN2D(input_shape, n_classes)\n",
        "model.compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "models = {}\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    n_classes = len(os.listdir(os.path.join(data_dir[0], i)))\n",
        "    models[i] = densenet(input_shape, n_classes)\n",
        "    models[i].compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    models[i].summary()\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training the Models\n",
        "\n",
        "# Train\n",
        "logdir='/content/drive/My Drive/Raga Identification/logs'\n",
        "tensorboard_callback1 = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "tensorboard_callback2 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0010, patience=5, verbose=1, mode='max', restore_best_weights=True, start_from_epoch=5)\n",
        "hist = model.fit(train, class_weight=class_weights, epochs=60, validation_data=val, callbacks=[tensorboard_callback1, tensorboard_callback2])\n",
        "print(hist.history)\n",
        "\n",
        "hists = {}\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    print(\"\\n\\n For \"+i)\n",
        "    tensorboard_callback1 = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "    tensorboard_callback2 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0010, patience=5, verbose=1, mode='max', restore_best_weights=True, start_from_epoch=5)\n",
        "    hists[i] = models[i].fit(trains[i], class_weight = class_weights_r[i], epochs=60, validation_data=vals[i], callbacks=[tensorboard_callback1, tensorboard_callback2])\n",
        "    print(hists[i].history)\n",
        "    break"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZhYBLbqvpJi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImYkSh830Ydf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Evaluating the Model\n",
        "\n",
        "''' Evaluation '''\n",
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
        "from sklearn.metrics import *\n",
        "\n",
        "print(\"For Model CRNN2D: \")\n",
        "loss, results = model.evaluate(test, batch_size=328, callbacks=[tensorboard_callback1, tensorboard_callback2])\n",
        "print(\"Result : \\n\", results)\n",
        "print(\"Loss: \\n\", loss)\n",
        "\n",
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = BinaryAccuracy()\n",
        "pre1 = Precision()\n",
        "re1 = Recall()\n",
        "acc1 = BinaryAccuracy()\n",
        "labels = []\n",
        "predicts = []\n",
        "for batch in test.as_numpy_iterator():\n",
        "    X, y = batch\n",
        "    labels.extend(list(y))\n",
        "    yhat = model.predict(X, verbose=0)\n",
        "    max = a = 0\n",
        "    arr = np.ones(yhat.shape[0])*-1\n",
        "    for i in yhat:\n",
        "        max = np.argmax(i)\n",
        "        arr[a] = max\n",
        "        a+=1\n",
        "    predicts.extend(list(arr))\n",
        "    pre1.update_state(y, arr)\n",
        "    re1.update_state(y, arr)\n",
        "    acc1.update_state(y, arr)\n",
        "print(f'Precision:{pre1.result().numpy()}, Recall:{re1.result().numpy()}, Accuracy:{acc1.result().numpy()}')\n",
        "matrix = confusion_matrix(y, arr)\n",
        "print(\"Confusion Matrix for DenseNet201: \\n\", matrix)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = os.listdir(data_dir[0]))\n",
        "display.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"For Test Dataset: \")\n",
        "pre.update_state(labels, predicts)\n",
        "re.update_state(labels, predicts)\n",
        "acc.update_state(labels, predicts)\n",
        "print(f'Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy (Binary):{acc.result().numpy()}')\n",
        "matrix = confusion_matrix(labels, predicts)\n",
        "print(\"Confusion Matrix for Model on Test Dataset: \\n\", matrix)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = os.listdir(data_dir[0]))\n",
        "display.plot()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "''' For the Ragas '''\n",
        "losses = {}\n",
        "results = {}\n",
        "skip = 9\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    losses[i], results[i] = models[i].evaluate(tests[i], batch_size=328, callbacks=[tensorboard_callback1, tensorboard_callback2])\n",
        "    print(\"For \", i, \" Result : \", results[i], \" Loss : \", losses[i])\n",
        "\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    print(\"\\n\\n For \"+i)\n",
        "    pre = Precision()\n",
        "    re = Recall()\n",
        "    acc = BinaryAccuracy()\n",
        "    pre1 = Precision()\n",
        "    re1 = Recall()\n",
        "    acc1 = BinaryAccuracy()\n",
        "    labels = []\n",
        "    predicts = []\n",
        "    for batch in tests[i].as_numpy_iterator():\n",
        "        X, y = batch\n",
        "        labels.extend(list(y))\n",
        "        yhat = models[i].predict(X)\n",
        "        max = a = 0\n",
        "        arr = np.ones(yhat.shape[0])*-1\n",
        "        for i in yhat:\n",
        "            max = np.argmax(i)\n",
        "            arr[a] = max\n",
        "            a+=1\n",
        "        predicts.extend(list(arr))\n",
        "        pre1.update_state(y, arr)\n",
        "        re1.update_state(y, arr)\n",
        "        acc1.update_state(y, arr)\n",
        "    print(f'Precission:{pre1.result().numpy()}, Recall:{re1.result().numpy()}, Accuracy:{acc1.result().numpy()}')\n",
        "    matrix = confusion_matrix(y, arr)\n",
        "    print(\"Confusion Matrix for \", i, \": \\n\", matrix)\n",
        "    display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = os.listdir(data_dir[0]+\"/\"+i))\n",
        "    display.plot()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"For Test Dataset: \")\n",
        "    pre.update_state(labels, predicts)\n",
        "    re.update_state(labels, predicts)\n",
        "    acc.update_state(labels, predicts)\n",
        "    print(f'Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy (Binary):{acc.result().numpy()}')\n",
        "    matrix = confusion_matrix(labels, predicts)\n",
        "    print(\"Confusion Matrix for Model on Test Dataset: \\n\", matrix)\n",
        "    display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = os.listdir(data_dir[0]))\n",
        "    display.plot()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Zs1iY6KGoxBd"
      },
      "outputs": [],
      "source": [
        "#@title Saving the Models and their Histories\n",
        "\n",
        "# Saving Model\n",
        "\n",
        "model.save(os.path.join('/content/drive/My Drive/Raga Identification/models','crnn2d_thaatclassifier.h5'))\n",
        "\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    filename = i+\".h5\"\n",
        "    models[i].save(os.path.join('/content/drive/My Drive/Raga Identification/models',filename))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nmdrAUlSg26P"
      },
      "outputs": [],
      "source": [
        "#@title Re-Loading the Models and their Histories\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Loading the Models\n",
        "\n",
        "model = load_model('/content/drive/My Drive/Raga Identification/models/crnn2d_thaatclassifier.h5')\n",
        "\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    filename = i+\".h5\"\n",
        "    models[i] = load_model('/content/drive/My Drive/Raga Identification/models/'+filename)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8O4Dbacmo4eE"
      },
      "outputs": [],
      "source": [
        "#@title Performance Graphs\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "''' Model 1 '''\n",
        "# Loss\n",
        "fig = plt.figure()\n",
        "plt.title(\"Model 1: Thaat Classifier\\n\\n\", loc='center', fontsize=26)\n",
        "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
        "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
        "fig.suptitle('Loss', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "# Accuracy\n",
        "fig = plt.figure()\n",
        "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
        "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
        "fig.suptitle('Accuracy', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "''' Rest of the models '''\n",
        "a = 1\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    title = \"Model \"+str(a)+\": \"+i+\"\\n\"\n",
        "    # Loss\n",
        "    fig = plt.figure()\n",
        "    plt.title(title, loc='center', fontsize=26)\n",
        "    plt.plot(hists[i].history['loss'], color='teal', label='loss')\n",
        "    plt.plot(hists[i].history['val_loss'], color='orange', label='val_loss')\n",
        "    fig.suptitle('Loss', fontsize=20)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "    # Accuracy\n",
        "    fig = plt.figure()\n",
        "    plt.plot(hists[i].history['accuracy'], color='teal', label='accuracy')\n",
        "    plt.plot(hists[i].history['val_accuracy'], color='orange', label='val_accuracy')\n",
        "    fig.suptitle('Accuracy', fontsize=20)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "    a+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVE6ZJ86vd6w"
      },
      "source": [
        "# **Evaluating and Testing the Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3rXsrzvxHCi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ROC-AUC Curves\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "import math, os\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "#ROC-AUC\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_roc(val_label,decision_val, caption='ROC Curve'):\n",
        "    num_classes=np.unique(val_label).shape[0]\n",
        "    classes = []\n",
        "    for i in range(num_classes):\n",
        "        classes.append(i)\n",
        "    plt.figure()\n",
        "    decision_val = label_binarize(decision_val, classes=classes)\n",
        "\n",
        "    if num_classes!=2:\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(num_classes):\n",
        "            y_val = label_binarize(val_label, classes=classes)\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_val[:, i], decision_val[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        for i in range(num_classes):\n",
        "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                           ''.format(i+1, roc_auc[i]))\n",
        "    else:\n",
        "        fpr,tpr,_ = roc_curve(val_label,decision_val, pos_label=1)\n",
        "        roc_auc = auc(fpr,tpr)*100\n",
        "        plt.plot(fpr,tpr,label='ROC curve (AUC=%0.2f)'%roc_auc)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(caption)\n",
        "    plt.legend(loc=\"lower right\",  bbox_to_anchor=(1, 0.5))\n",
        "    plt.savefig(str(len(classes))+'.png',dpi=300)\n",
        "\n",
        "def predicting(ensemble_prob):\n",
        "    prediction = np.zeros((ensemble_prob.shape[0],))\n",
        "    for i in range(ensemble_prob.shape[0]):\n",
        "        temp = ensemble_prob[i]\n",
        "        t = np.where(temp == np.max(temp))[0][0]\n",
        "        prediction[i] = t\n",
        "    return prediction\n",
        "\n",
        "def metrics(labels, predictions, classes, l):\n",
        "    print(\"Classification Report:\")\n",
        "    print(\"Labels : \",labels.shape, '\\nPredictions : ', predictions.shape)\n",
        "    print(classification_report(labels, predictions, target_names = classes, digits = 4))\n",
        "    matrix = confusion_matrix(labels, predictions)\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(matrix)\n",
        "    display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = l)\n",
        "    display.plot()\n",
        "    plt.show()\n",
        "    print(\"\\nClasswise Accuracy :{}\".format(matrix.diagonal()/matrix.sum(axis = 1)))\n",
        "    print(\"\\nBalanced Accuracy Score: \",balanced_accuracy_score(labels,predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INrlA-O0qHR-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Testing the Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"For Model CRNN2D: \")\n",
        "labels = []\n",
        "predictions = []\n",
        "for batch1 in test.as_numpy_iterator():\n",
        "    X1, y1 = batch1\n",
        "    labels.extend(list(y1))\n",
        "    yhat1 = model.predict(X1, verbose=0)\n",
        "    max = a = 0\n",
        "    arr = np.ones(yhat1.shape[0])*-1\n",
        "    for i in yhat1:\n",
        "        max = np.argmax(i)\n",
        "        arr[a] = max\n",
        "        a+=1\n",
        "    predictions.extend(list(arr))\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "predictions = np.asarray(predictions)\n",
        "\n",
        "\n",
        "metrics(labels, predictions, ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])\n",
        "\n",
        "plot_roc(labels, predictions)\n",
        "\n",
        "\n",
        "avg_acc_list = []\n",
        "avg_precision_list = []\n",
        "avg_recall_list = []\n",
        "avg_f1_list = []\n",
        "\n",
        "acc_fold = accuracy_score(labels, predictions)\n",
        "avg_acc_list.append(acc_fold)\n",
        "precision_fold = precision_score(labels, predictions, average='macro')\n",
        "avg_precision_list.append(precision_fold)\n",
        "recall_fold = recall_score(labels, predictions, average='macro')\n",
        "avg_recall_list.append(recall_fold)\n",
        "f1_fold  = f1_score(labels, predictions, average='macro')\n",
        "avg_f1_list.append(f1_fold)\n",
        "\n",
        "print('Accuracy[{:.4f}] Precision[{:.4f}] Recall[{:.4f}] F1[{:.4f}] ]'.format(acc_fold, precision_fold, recall_fold, f1_fold ))\n",
        "print('________________________________________________________________')\n",
        "\n",
        "avg_acc = np.asarray(avg_acc_list)\n",
        "avg_pre = np.asarray(avg_precision_list)\n",
        "avg_recall = np.asarray(avg_recall_list)\n",
        "avg_f1 = np.asarray(avg_f1_list)\n",
        "print(\"\\n\")\n",
        "print('Overall Accuracy[{:.4f}] Overall Precision[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_pre), np.mean(avg_recall), np.mean(avg_f1)))\n",
        "\n",
        "\n",
        "''' For Ragas '''\n",
        "for i in os.listdir(data_dir[0]):\n",
        "    print(\"For \"+i+\": \")\n",
        "    labels = []\n",
        "    predictions = []\n",
        "    for batch1 in tests[i].as_numpy_iterator():\n",
        "        X1, y1 = batch1\n",
        "        labels.extend(list(y1))\n",
        "        yhat1 = models[i].predict(X1, verbose=0)\n",
        "        max = a = 0\n",
        "        arr = np.ones(yhat1.shape[0])*-1\n",
        "        for i in yhat1:\n",
        "            max = np.argmax(i)\n",
        "            arr[a] = max\n",
        "            a+=1\n",
        "        predictions.extend(list(arr))\n",
        "\n",
        "    labels = np.asarray(labels)\n",
        "    predictions = np.asarray(predictions)\n",
        "\n",
        "    classes = [z for z, p in enumerate(os.listdir(os.path.join(data_dir[0], i)))]\n",
        "    classes = np.array(classes)\n",
        "    classes+=1\n",
        "\n",
        "    metrics(labels, predictions, classes)\n",
        "\n",
        "    plot_roc(labels, predictions)\n",
        "\n",
        "\n",
        "    avg_acc_list = []\n",
        "    avg_precision_list = []\n",
        "    avg_recall_list = []\n",
        "    avg_f1_list = []\n",
        "\n",
        "    acc_fold = accuracy_score(labels, predictions)\n",
        "    avg_acc_list.append(acc_fold)\n",
        "    precision_fold = precision_score(labels, predictions, average='macro')\n",
        "    avg_precision_list.append(precision_fold)\n",
        "    recall_fold = recall_score(labels, predictions, average='macro')\n",
        "    avg_recall_list.append(recall_fold)\n",
        "    f1_fold  = f1_score(labels, predictions, average='macro')\n",
        "    avg_f1_list.append(f1_fold)\n",
        "\n",
        "    print('Accuracy[{:.4f}] Precision[{:.4f}] Recall[{:.4f}] F1[{:.4f}] ]'.format(acc_fold, precision_fold, recall_fold, f1_fold ))\n",
        "    print('________________________________________________________________')\n",
        "\n",
        "    avg_acc = np.asarray(avg_acc_list)\n",
        "    avg_pre = np.asarray(avg_precision_list)\n",
        "    avg_recall = np.asarray(avg_recall_list)\n",
        "    avg_f1 = np.asarray(avg_f1_list)\n",
        "    print(\"\\n\")\n",
        "    print('Overall Accuracy[{:.4f}] Overall Precision[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_pre), np.mean(avg_recall), np.mean(avg_f1)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "aQ-AU5TlDFP3",
        "9zHp0WVQDl24",
        "1iu9_dwODdj-",
        "1QjikBwbEKuG",
        "NVE6ZJ86vd6w"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}